{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added notebook directory to Python path:\n",
      "   /workspaces/llmops-course/modules/experiment_tracking/solutions\n",
      "   You can now import modules from this directory\n",
      "‚ö†Ô∏è Warning: Failed to set up notebook path\n",
      "üîÑ Initializing Course environment...\n",
      "üîÅ Autoreload enabled: modules will reload automatically when changed\n",
      "üîï Suppressed future deprecation warnings\n",
      "üìù Logging configured\n",
      "üìä Pandas display settings configured for better output\n",
      "üîç Looking for .env file at: /workspaces/llmops-course/.env\n",
      "‚úÖ Successfully loaded environment variables from /workspaces/llmops-course/.env\n",
      "üìã Loaded variables: GEMINI_API_KEY=****sbqo\n",
      "‚öôÔ∏è Disabled MLflow system metrics logging\n",
      "üìî Disabled MLflow notebook display (avoids VSCode bugs)\n",
      "\n",
      "================================================================================\n",
      "üéâ All systems go! Your Course environment is ready for learning!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import helpers\n",
    "helpers.initialize(notebook_path=__vsc_ipynb_file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/workspaces/llmops-course/.mlflow_data/artifacts/1', creation_time=1740732634531, experiment_id='1', last_update_time=1740732634531, lifecycle_stage='active', name='Test Summarizer', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5050\")\n",
    "mlflow.set_experiment(\"Test Summarizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "\n",
      "<body>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textsummarizer import TextSummarizer\n",
    "\n",
    "prompt = \"Summarize the following text: {text}\"\n",
    "summarizer = TextSummarizer(prompt = prompt, max_output_tokens=1000)\n",
    "\n",
    "text = open('assets/articles_short/ai_relationships.html').read()\n",
    "print(text[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic MLflow functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run My first run at: http://localhost:5050/#/experiments/1/runs/554f4786b7704586892f1381ec7bf173\n",
      "üß™ View experiment at: http://localhost:5050/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"My first run\"):\n",
    "    mlflow.log_param(\"max_output_tokens\", 1000)\n",
    "    mlflow.log_param(\"prompt\", prompt)\n",
    "    mlflow.log_text(text, \"original_text.txt\")\n",
    "\n",
    "    summary = summarizer.summarize(text)\n",
    "\n",
    "    mlflow.log_text(summary, \"summary.txt\")\n",
    "\n",
    "    mlflow.log_metric(\"summary_length\", len(summary))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracing and Autologging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run Tracing and Autologging run at: http://localhost:5050/#/experiments/1/runs/6e1aac527ac44dfcafff9fd8bb690a12\n",
      "üß™ View experiment at: http://localhost:5050/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "import mlflow.gemini\n",
    "mlflow.gemini.autolog()\n",
    "\n",
    "with mlflow.start_run(run_name=\"Tracing and Autologging run\"):\n",
    "    path = 'assets/articles_short/ai_relationships.html'\n",
    "    text = open(path).read()\n",
    "\n",
    "    mlflow.log_param(\"prompt\", prompt)\n",
    "    mlflow.log_param(\"path\", path)\n",
    "    mlflow.log_text(text, \"original_text.txt\")\n",
    "\n",
    "    summary = summarizer.summarize(text)\n",
    "\n",
    "    mlflow.log_text(summary, \"summary.txt\")\n",
    "\n",
    "    mlflow.log_metric(\"summary_length\", len(summary))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a full-length article?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run Tracing and Autologging run at: http://localhost:5050/#/experiments/1/runs/437b930519fe494b8e5ee265763ef116\n",
      "üß™ View experiment at: http://localhost:5050/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"Full Article Tracing and Autologging\"):\n",
    "    path = 'assets/articles_full_length/ai_relationships.html'\n",
    "    text = open(path).read()\n",
    "\n",
    "    mlflow.log_param(\"max_output_tokens\", 1000)\n",
    "    mlflow.log_param(\"prompt\", prompt)\n",
    "    mlflow.log_param(\"path\", path)\n",
    "    mlflow.log_text(text, \"original_text.txt\")\n",
    "\n",
    "    summary = summarizer.summarize(text)\n",
    "\n",
    "    mlflow.log_text(summary, \"summary.txt\")\n",
    "\n",
    "    mlflow.log_metric(\"summary_length\", len(summary))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a Rouge scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run Rouge Scorer at: http://localhost:5050/#/experiments/1/runs/2eac3a109d824c6ebe3f2ecd3bd94e5e\n",
      "üß™ View experiment at: http://localhost:5050/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def rouge_metrics(reference, prediction):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, prediction)\n",
    "    \n",
    "    return scores['rouge1'].fmeasure\n",
    "\n",
    "with mlflow.start_run(run_name=\"Rouge Scorer\"):\n",
    "    path = 'assets/articles_full_length/ai_relationships.html'\n",
    "    text = open(path).read()\n",
    "\n",
    "    mlflow.log_param(\"max_output_tokens\", 1000)\n",
    "    mlflow.log_param(\"prompt\", prompt)\n",
    "    mlflow.log_param(\"path\", path)\n",
    "    mlflow.log_text(text, \"original_text.txt\")\n",
    "\n",
    "    summary = summarizer.summarize(text)\n",
    "\n",
    "    mlflow.log_text(summary, \"summary.txt\")\n",
    "\n",
    "    mlflow.log_metric(\"summary_length\", len(summary))\n",
    "\n",
    "    rouge_f1 = rouge_metrics(text, summary)\n",
    "    mlflow.log_metric(\"rouge1_f1\", rouge_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
